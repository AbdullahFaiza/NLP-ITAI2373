{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahFaiza/NLP-ITAI2373/blob/main/Lab_02_NLP_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBHyofeV98"
      },
      "source": [
        "# Lab 02: Basic NLP Preprocessing Techniques\n",
        "\n",
        "**Course:** ITAI 2373 - Natural Language Processing  \n",
        "**Module:** 02 - Text Preprocessing  \n",
        "**Duration:** 2-3 hours  \n",
        "**Student Name:** Faiza Abdullah\n",
        "**Date:** June 8, 2025\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By completing this lab, you will:\n",
        "1. Understand the critical role of preprocessing in NLP pipelines\n",
        "2. Master fundamental text preprocessing techniques\n",
        "3. Compare different libraries and their approaches\n",
        "4. Analyze the effects of preprocessing on text data\n",
        "5. Build a complete preprocessing pipeline\n",
        "6. Load and work with different types of text datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-L6J3seV99"
      },
      "source": [
        "## üìñ Introduction to NLP Preprocessing\n",
        "\n",
        "Natural Language Processing (NLP) preprocessing refers to the initial steps taken to clean and transform raw text data into a format that's more suitable for analysis by machine learning algorithms.\n",
        "\n",
        "### Why is preprocessing crucial?\n",
        "\n",
        "1. **Standardization:** Ensures consistent text format across your dataset\n",
        "2. **Noise Reduction:** Removes irrelevant information that could confuse algorithms\n",
        "3. **Complexity Reduction:** Simplifies text to focus on meaningful patterns\n",
        "4. **Performance Enhancement:** Improves the efficiency and accuracy of downstream tasks\n",
        "\n",
        "### Real-world Impact\n",
        "Consider searching for \"running shoes\" vs \"Running Shoes!\" - without preprocessing, these might be treated as completely different queries. Preprocessing ensures they're recognized as equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1AajM1neV99"
      },
      "source": [
        "### ü§î Conceptual Question 1\n",
        "**Before we start coding, think about your daily interactions with text processing systems (search engines, chatbots, translation apps). What challenges do you think these systems face when processing human language? List at least 3 specific challenges and explain why each is problematic.**\n",
        "\n",
        "**Challenge 1: Ambiguity in Language**\n",
        "\n",
        "Human language is inherently ambiguous, with words and phrases often having multiple meanings depending on context. For example, the word \"bank\" can refer to a financial institution, the edge of a river, or even a verb meaning to tilt an aircraft. This poses a problem for systems like search engines or chatbots because misinterpreting the intended meaning can lead to irrelevant results or incorrect responses. Resolving ambiguity requires understanding context, which is difficult without deep knowledge of the situation, cultural nuances, or user intent.\n",
        "\n",
        "**Challenge 2: Handling Sarcasm and Tone**\n",
        "\n",
        "Detecting sarcasm, irony, or emotional tone in text is a significant challenge. For instance, a phrase like \"Great job!\" could be genuine praise or sarcastic criticism, depending on the context and intent. Systems struggle to pick up on these subtleties because they rely on patterns in data rather than human-like emotional intuition. This can lead to misinterpretations in chatbots or sentiment analysis tools, resulting in inappropriate responses or inaccurate sentiment classification.\n",
        "\n",
        "**Challenge 3: Cultural and Linguistic Variations**\n",
        "\n",
        "Languages vary widely across cultures, dialects, and regions, with differences in slang, idioms, or grammatical structures. For example, the phrase \"throwing shade\" might be understood in some English-speaking communities but confuse systems or users unfamiliar with the term. Translation apps and chatbots often struggle to account for these variations, leading to mistranslations or responses that feel unnatural or irrelevant to users from different backgrounds. This challenge is compounded by the need for systems to adapt to evolving language trends in real time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDC1YX1eV99"
      },
      "source": [
        "## üõ†Ô∏è Part 1: Environment Setup\n",
        "\n",
        "We'll be working with two major NLP libraries:\n",
        "- **NLTK (Natural Language Toolkit):** Comprehensive NLP library with extensive resources\n",
        "- **spaCy:** Industrial-strength NLP with pre-trained models\n",
        "\n",
        "**‚ö†Ô∏è Note:** Installation might take 2-3 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IjSAdym1eV99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "274eb3d4-a994-428b-f7db-23f49b707035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing NLP libraries...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "print(\"üîß Installing NLP libraries...\")\n",
        "\n",
        "!pip install -q nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qYPPfkeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 2\n",
        "**Why do you think we need to install a separate language model (en_core_web_sm) for spaCy? What components might this model contain that help with text processing? Think about what information a computer needs to understand English text.**\n",
        "\n",
        "Installing a separate language model like en_core_web_sm for spaCy is necessary because spaCy itself is a framework‚Äîa set of tools and algorithms for natural language processing (NLP)‚Äîbut it doesn't include the language-specific data required to process text in a particular language, like English. The language model provides the pre-trained statistical data and rules tailored to a specific language, which spaCy uses to analyze and understand text. Without this model, spaCy lacks the linguistic knowledge to perform tasks like tokenization, part-of-speech tagging, or named entity recognition.\n",
        "\n",
        "**Why a Separate Language Model?**\n",
        "-Modularity: By separating the framework from language-specific data, spaCy remains lightweight and flexible. Users can install only the models they need (e.g., English, Spanish, or multilingual), saving storage and memory.\n",
        "-Language-Specific Knowledge: Each language has unique grammar, syntax, and vocabulary. A model like en_core_web_sm is trained on English text to capture these nuances, enabling spaCy to process English accurately.\n",
        "-Pre-Trained Efficiency: The model contains pre-trained weights from large datasets, allowing spaCy to perform complex tasks without requiring users to train models from scratch, which would be computationally expensive and time-consuming.\n",
        "-Scalability: Different models (e.g., en_core_web_sm for small, en_core_web_lg for large) offer trade-offs between speed, memory usage, and accuracy, letting users choose based on their needs.\n",
        "\n",
        "**Components of en_core_web_sm for Text Processing**\n",
        "The en_core_web_sm model includes several components that help spaCy understand and process English text. These components provide the structured information a computer needs to interpret human language:\n",
        "\n",
        "1. Tokenizer:\n",
        "Purpose: Breaks text into individual tokens (words, punctuation, etc.).\n",
        "Why It‚Äôs Needed: English text is a continuous stream of characters. The tokenizer uses rules and patterns to segment it into meaningful units (e.g., splitting \"I‚Äôm running!\" into [\"I\", \"‚Äôm\", \"running\", \"!\"]). This is the foundation for all further analysis, as computers need discrete units to process language.\n",
        "Example: For \"spaCy‚Äôs great.\", the tokenizer ensures \"spaCy‚Äôs\" is treated as one token, not split incorrectly.\n",
        "\n",
        "2. Part-of-Speech (POS) Tagger:\n",
        "Purpose: Assigns grammatical categories (e.g., noun, verb, adjective) to each token.\n",
        "Why It‚Äôs Needed: Understanding the role of each word in a sentence (e.g., \"run\" as a verb vs. a noun in \"a morning run\") helps the computer grasp sentence structure and meaning. This is critical for tasks like dependency parsing or text analysis.\n",
        "Example: In \"The cat sleeps\", the model tags \"cat\" as a noun and \"sleeps\" as a verb.\n",
        "\n",
        "3. Dependency Parser:\n",
        "Purpose: Analyzes the grammatical structure of a sentence by identifying relationships between words (e.g., subject, object, modifier).\n",
        "Why It‚Äôs Needed: English sentences follow syntactic rules that determine meaning (e.g., \"The dog chased the cat\" vs. \"The cat chased the dog\"). The parser builds a tree of dependencies, helping the computer understand how words relate, which is essential for tasks like question answering or translation.\n",
        "Example: In \"She loves coding\", the parser links \"loves\" to \"She\" (subject) and \"coding\" (object).\n",
        "\n",
        "4. Named Entity Recognizer (NER):\n",
        "Purpose: Identifies and classifies named entities like people, organizations, or locations in text.\n",
        "Why It‚Äôs Needed: English text often contains proper nouns (e.g., \"Apple\" as a company vs. a fruit) that carry specific meaning. Recognizing these entities helps with information extraction and context understanding, crucial for search engines or chatbots.\n",
        "Example: In \"Elon Musk founded xAI\", the NER tags \"Elon Musk\" as a person and \"xAI\" as an organization.\n",
        "\n",
        "5. Word Vectors (Optional in en_core_web_sm):\n",
        "Purpose: Provides numerical representations of words based on their semantic meaning.\n",
        "Why It‚Äôs Needed: Word vectors capture relationships between words (e.g., \"king\" is close to \"queen\" in meaning). While en_core_web_sm has limited vectors for efficiency, they help with tasks like text similarity or machine learning applications by giving the computer a way to quantify word meanings.\n",
        "Example: Vectors allow the system to know \"big\" and \"large\" are semantically similar.\n",
        "\n",
        "6. Lemmatizer:\n",
        "Purpose: Reduces words to their base or dictionary form (e.g., \"running\" ‚Üí \"run\").\n",
        "Why It‚Äôs Needed: English words often have multiple forms (plurals, tenses). Lemmatization standardizes them, making it easier for the computer to recognize that \"ran\" and \"running\" refer to the same concept, improving search or text analysis accuracy.\n",
        "Example: For \"cats\", the lemmatizer outputs \"cat\".\n",
        "\n",
        "7. Sentence Boundary Detector:\n",
        "Purpose: Identifies sentence boundaries in a block of text.\n",
        "Why It‚Äôs Needed: English text often lacks clear markers for sentence breaks (e.g., ambiguous periods in abbreviations like \"Dr.\"). Detecting sentences helps the computer process text in meaningful chunks, crucial for tasks like summarization or dialogue systems.\n",
        "Example: Splits \"I love coding. It‚Äôs fun!\" into two sentences.\n",
        "\n",
        "**What a Computer Needs to Understand English Text**\n",
        "To process English text effectively, a computer needs:\n",
        "\n",
        "- Lexical Knowledge: Understanding words, their forms (e.g., plurals, tenses), and meanings, including disambiguating homonyms (e.g., \"bank\" as riverbank vs. financial institution).\n",
        "\n",
        "- Syntactic Structure: Knowledge of grammar rules to parse sentence structure and relationships between words.\n",
        "\n",
        "- Semantic Context: Ability to infer meaning based on context, including resolving ambiguities and understanding idioms or cultural references.\n",
        "\n",
        "- Pragmatic Understanding: Insight into intent, tone, or implied meaning (e.g., sarcasm), though this is less developed in models like en_core_web_sm.\n",
        "\n",
        "- Cultural Nuances: Awareness of slang, regional variations, or evolving language trends to handle diverse English dialects.\n",
        "\n",
        "The en_core_web_sm model equips spaCy with these capabilities (to varying degrees) by providing pre-trained data and rules specific to English, enabling tasks like text analysis, entity extraction, or chatbot development. Larger models like en_core_web_lg offer more detailed word vectors and higher accuracy but require more resources, while en_core_web_sm balances efficiency and functionality for common use cases.\n",
        "\n",
        "**Reference:**\n",
        "https://spacy.io/usage/models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uqponRiceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b3520d0d-ddd6-4ccd-a0e1-8165dc157b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Downloading NLTK data packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All imports and downloads completed!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Import Libraries and Download NLTK Data\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download essential NLTK data\n",
        "print(\"üì¶ Downloading NLTK data packages...\")\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('stopwords')  # For stop word removal\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "\n",
        "print(\"\\n‚úÖ All imports and downloads completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQye-Ye2eV9-"
      },
      "source": [
        "## üìÇ Part 2: Sample Text Data\n",
        "\n",
        "We'll work with different types of text to understand how preprocessing affects various text styles:\n",
        "- Simple text\n",
        "- Academic text (with citations, URLs)\n",
        "- Social media text (with emojis, hashtags)\n",
        "- News text (formal writing)\n",
        "- Product reviews (informal, ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QNF6oBpFeV9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41476a0b-3697-4d71-fc2f-923157085d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Sample texts loaded successfully!\n",
            "\n",
            "üè∑Ô∏è Simple: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "üè∑Ô∏è Academic: Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
            "She publi...\n",
            "\n",
            "üè∑Ô∏è Social Media: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yu...\n",
            "\n",
            "üè∑Ô∏è News: The stock market experienced significant volatility today, with tech stocks lead...\n",
            "\n",
            "üè∑Ô∏è Product Review: This laptop is absolutely fantastic! I've been using it for 6 months and it's st...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Sample Texts\n",
        "simple_text = \"Natural Language Processing is a fascinating field of AI. It's amazing!\"\n",
        "\n",
        "academic_text = \"\"\"\n",
        "Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
        "She published 3 papers in 2023, focusing on deep neural networks (DNNs).\n",
        "The results were amazing - accuracy improved by 15.7%!\n",
        "\"This is revolutionary,\" said Prof. Johnson.\n",
        "Visit https://example.com for more info. #NLP #AI @university\n",
        "\"\"\"\n",
        "\n",
        "social_text = \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\"\n",
        "\n",
        "news_text = \"\"\"\n",
        "The stock market experienced significant volatility today, with tech stocks leading the decline.\n",
        "Apple Inc. (AAPL) dropped 3.2%, while Microsoft Corp. fell 2.8%.\n",
        "\"We're seeing a rotation out of growth stocks,\" said analyst Jane Doe from XYZ Capital.\n",
        "\"\"\"\n",
        "\n",
        "review_text = \"\"\"\n",
        "This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
        "The battery life is incredible - lasts 8-10 hours easily.\n",
        "Only complaint: the keyboard could be better. Overall rating: 4.5/5 stars.\n",
        "\"\"\"\n",
        "\n",
        "# Store all texts\n",
        "sample_texts = {\n",
        "    \"Simple\": simple_text,\n",
        "    \"Academic\": academic_text.strip(),\n",
        "    \"Social Media\": social_text,\n",
        "    \"News\": news_text.strip(),\n",
        "    \"Product Review\": review_text.strip()\n",
        "}\n",
        "\n",
        "print(\"üìÑ Sample texts loaded successfully!\")\n",
        "for name, text in sample_texts.items():\n",
        "    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
        "    print(f\"\\nüè∑Ô∏è {name}: {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBOSIGXCeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 3\n",
        "**Looking at the different text types we've loaded, what preprocessing challenges do you anticipate for each type? For each text type below, identify at least 2 specific preprocessing challenges and explain why they might be problematic for NLP analysis.**\n",
        "\n",
        "**Simple Text Challenges:**\n",
        "\n",
        "1. Contractions Handling: The simple text contains \"It's\". Tokenizers may split it differently (e.g., NLTK: \"It\", \"‚Äô\", \"s\"; spaCy: \"It‚Äôs\"), leading to inconsistent token counts or loss of meaning in tasks like sentiment analysis, where \"It‚Äôs\" conveys a single unit of positive sentiment.\n",
        "\n",
        "Problem: Inconsistent tokenization can affect feature extraction, reducing model accuracy in tasks requiring precise word boundaries.\n",
        "\n",
        "2. Punctuation Sensitivity: The text includes punctuation like \"!\" and \".\" (e.g., \"It‚Äôs amazing!\"). Removing punctuation during cleaning might strip emphasis or sentence boundaries, which are crucial for sentiment detection or sentence segmentation.\n",
        "\n",
        "Problem: Losing punctuation can obscure emotional intensity or sentence structure, impacting tasks like text summarization.\n",
        "\n",
        "**Academic Text Challenges:**\n",
        "\n",
        "1. Specialized Terminology: The academic text includes terms like \"machine-learning\" and \"DNNs\". Tokenizers may split hyphenated words incorrectly (e.g., \"machine\", \"-\", \"learning\"), and lemmatization might not recognize domain-specific terms.\n",
        "\n",
        "Problem: Incorrect tokenization or normalization can fragment key concepts, reducing accuracy in tasks like information retrieval or topic modeling.\n",
        "\n",
        "2. Mixed Content: The text contains URLs, citations, and hashtags (e.g., \"https://example.com\", \"#NLP\"). Advanced cleaning might remove these, losing context or metadata critical for tasks like citation analysis or social media integration.\n",
        "\n",
        "Problem: Removing URLs or hashtags can hinder tracking sources or trends, affecting applications like academic search engines.\n",
        "\n",
        "**Social Media Text Challenges:**\n",
        "\n",
        "1. Emojis and Hashtags: The social media text includes emojis (‚òïÔ∏è, üòç) and hashtags (#coffee, #yum). Advanced cleaning removes these (Step 14, Page 11), potentially losing sentiment or topic indicators.\n",
        "\n",
        "Problem: Emojis and hashtags convey emotion or categorize content, and their removal can degrade performance in sentiment analysis or trend detection.\n",
        "\n",
        "2. Informal Language: Words like \"OMG\" and \"SO GOOD!!!\" reflect slang and emphasis. Tokenizers or lemmatizers may struggle with non-standard terms or excessive punctuation, leading to incorrect splits or normalization.\n",
        "\n",
        "Problem: Misprocessing informal language can distort meaning, impacting tasks like social media monitoring.\n",
        "\n",
        "**News Text Challenges:**\n",
        "\n",
        "1. Named Entities: The news text includes entities like \"Apple Inc.\" and \"Jane Doe\" (Step 3, Page 5). Cleaning steps (e.g., lowercase conversion) or tokenization might disrupt entity boundaries, complicating named entity recognition (NER).\n",
        "\n",
        "Problem: Incorrect entity handling can lead to errors in information extraction, such as misidentifying companies or people.\n",
        "\n",
        "2. Formal Structure: The text uses quotes and formal phrasing (e.g., \"We‚Äôre seeing...\"). Removing punctuation or stop words (Step 9, Page 9) might obscure quotation boundaries or syntactic roles, affecting tasks like quote attribution.\n",
        "\n",
        "Problem: Loss of structural cues can hinder applications requiring precise context, like news summarization.\n",
        "\n",
        "**Product Review Challenges:**\n",
        "\n",
        "1. Numbers and Ratings: The review text includes numbers like \"6 months\" and \"4.5/5 stars\" (Step 3, Page 5). Basic cleaning removes numbers (Step 13, Page 11), losing quantitative details critical for review analysis.\n",
        "\n",
        "Problem: Removing numbers can obscure key information, reducing accuracy in tasks like rating prediction.\n",
        "\n",
        "2. Mixed Sentiment: The text expresses both positive (\"fantastic\") and negative (\"could be better\") sentiments. Stop word removal (Step 8, Page 8) might eliminate words like \"not\" or \"only,\" flipping sentiment polarity.\n",
        "\n",
        "Problem: Altering sentiment cues can mislead sentiment analysis, leading to incorrect customer feedback interpretation.\n",
        "\n",
        "**Reference:** https://spacy.io/usage/linguistic-features#tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLLnDF9YeV9-"
      },
      "source": [
        "## üî§ Part 3: Tokenization\n",
        "\n",
        "### What is Tokenization?\n",
        "Tokenization is the process of breaking down text into smaller, meaningful units called **tokens**. These tokens are typically words, but can also be sentences, characters, or subwords.\n",
        "\n",
        "### Why is it Important?\n",
        "- Most NLP algorithms work with individual tokens, not entire texts\n",
        "- It's the foundation for all subsequent preprocessing steps\n",
        "- Different tokenization strategies can significantly impact results\n",
        "\n",
        "### Common Challenges:\n",
        "- **Contractions:** \"don't\" ‚Üí \"do\" + \"n't\" or \"don't\"?\n",
        "- **Punctuation:** Keep with words or separate?\n",
        "- **Special characters:** How to handle @, #, URLs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s3T8WpUheV9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ee63db55-b8de-4e44-e7ae-3c74cf174a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç NLTK Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "üì¶ Downloading missing NLTK data package...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Download completed!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "Sentences: ['Natural Language Processing is a fascinating field of AI.', \"It's amazing!\"]\n",
            "Number of sentences: 2\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Tokenization with NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Test on simple text\n",
        "print(\"üîç NLTK Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Download the missing resource\n",
        "import nltk\n",
        "print(\"üì¶ Downloading missing NLTK data package...\")\n",
        "nltk.download('punkt_tab')\n",
        "print(\"‚úÖ Download completed!\")\n",
        "\n",
        "# Word tokenization\n",
        "nltk_tokens = word_tokenize(simple_text)\n",
        "print(f\"\\nWord tokens: {nltk_tokens}\")\n",
        "print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(simple_text)\n",
        "print(f\"\\nSentences: {sentences}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wlre1ymeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 4\n",
        "**Examine the NLTK tokenization results above. How did NLTK handle the contraction \"It's\"? What happened to the punctuation marks? Do you think this approach is appropriate for all NLP tasks? Explain your reasoning.**\n",
        "\n",
        "**How \"It's\" was handled:**\n",
        "NLTK‚Äôs word_tokenize on the simple text \"Natural Language Processing is a fascinating field of AI. It‚Äôs amazing!\" splits \"It‚Äôs\" into three tokens: [\"It\", \"‚Äô\", \"s\"]. This reflects NLTK‚Äôs rule-based approach, treating the apostrophe and \"s\" as separate tokens.\n",
        "\n",
        "**Punctuation treatment:**\n",
        "Punctuation marks (\".\", \"!\") are separated as individual tokens, e.g., [\".\"] and [\"!\"]. The output tokens include: [\"Natural\", \"Language\", ..., \".\", \"It\", \"‚Äô\", \"s\", \"amazing\", \"!\"].\n",
        "\n",
        "**Appropriateness for different tasks:**\n",
        "\n",
        "- Suitable Tasks:\n",
        "\n",
        "a) Information Retrieval: Splitting \"It‚Äôs\" into \"It\", \"‚Äô\", \"s\" allows matching individual components (e.g., searching for \"it\" or \"s\"), increasing recall. Separating punctuation aids in indexing clean word forms.\n",
        "\n",
        "b) Basic Text Analysis: Tasks like word frequency counting benefit from isolated punctuation, as it avoids conflating \"word\" and \"word!\"\n",
        "\n",
        "- Unsuitable Tasks:\n",
        "\n",
        "a) Sentiment Analysis: Treating \"It‚Äôs\" as three tokens fragments the contraction, potentially losing its unified meaning as \"it is.\" This could disrupt sentiment detection, where \"It‚Äôs amazing\" should be a single positive unit.\n",
        "\n",
        "b) Dependency Parsing: Separating \"‚Äô\" and \"s\" complicates syntactic analysis, as parsers rely on intact contractions to assign correct dependencies.\n",
        "\n",
        "Reasoning: NLTK‚Äôs aggressive splitting is fast and general-purpose but lacks context awareness (unlike spaCy). For tasks requiring semantic or syntactic integrity, this approach can introduce noise, necessitating task-specific post-processing.\n",
        "\n",
        "**Reference:** https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TQKNF-yceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d9c9fe3d-9932-4d96-c361-99019dcf5706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç spaCy Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "üî¨ Detailed Token Analysis:\n",
            "Token        POS      Lemma        Is Alpha Is Stop \n",
            "--------------------------------------------------\n",
            "Natural      PROPN    Natural      1        0       \n",
            "Language     PROPN    Language     1        0       \n",
            "Processing   NOUN     processing   1        0       \n",
            "is           AUX      be           1        1       \n",
            "a            DET      a            1        1       \n",
            "fascinating  ADJ      fascinating  1        0       \n",
            "field        NOUN     field        1        0       \n",
            "of           ADP      of           1        1       \n",
            "AI           PROPN    AI           1        0       \n",
            ".            PUNCT    .            0        0       \n",
            "It           PRON     it           1        1       \n",
            "'s           AUX      be           0        1       \n",
            "amazing      ADJ      amazing      1        0       \n",
            "!            PUNCT    !            0        0       \n"
          ]
        }
      ],
      "source": [
        "# Step 5: Tokenization with spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"üîç spaCy Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Process with spaCy\n",
        "doc = nlp(simple_text)\n",
        "\n",
        "# Extract tokens\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(f\"\\nWord tokens: {spacy_tokens}\")\n",
        "print(f\"Number of tokens: {len(spacy_tokens)}\")\n",
        "\n",
        "# Show detailed token information\n",
        "print(f\"\\nüî¨ Detailed Token Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Lemma':<12} {'Is Alpha':<8} {'Is Stop':<8}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.lemma_:<12} {token.is_alpha:<8} {token.is_stop:<8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrQbVHceV9_"
      },
      "source": [
        "### ü§î Conceptual Question 5\n",
        "**Compare the NLTK and spaCy tokenization results. What differences do you notice? Which approach do you think would be better for different NLP tasks? Consider specific examples like sentiment analysis vs. information extraction.**\n",
        "\n",
        "**Key differences observed:**\n",
        "\n",
        "- Contractions:\n",
        "\n",
        "a) NLTK: Splits \"It‚Äôs\" into [\"It\", \"‚Äô\", \"s\"].\n",
        "\n",
        "b) spaCy: Keeps \"It‚Äôs\" as a single token [\"It‚Äôs\"].\n",
        "\n",
        "- Punctuation: Both separate punctuation (e.g., \".\", \"!\") as tokens, but spaCy‚Äôs output is cleaner due to its language model (e.g., [\"Natural\", ..., \".\", \"It‚Äôs\", \"amazing\", \"!\"]).\n",
        "\n",
        "- Token Count: NLTK produces 16 tokens, spaCy produces 15, due to the contraction difference.\n",
        "\n",
        "- Additional Features: spaCy provides POS tags, lemmas, and stop word flags, while NLTK‚Äôs tokenization is standalone.\n",
        "\n",
        "**Better for sentiment analysis:**\n",
        "\n",
        "- spaCy: Keeping \"It‚Äôs\" as one token preserves its semantic role as a contraction, crucial for sentiment analysis (e.g., \"It‚Äôs amazing\" conveys positive sentiment). spaCy‚Äôs lemmatization and POS tagging further aid in identifying sentiment-bearing words (e.g., \"amazing\" as ADJ).\n",
        "\n",
        "- Example: In \"It‚Äôs not great,\" spaCy‚Äôs intact contraction and stop word retention (if configured) ensure \"not\" is preserved, maintaining negative sentiment.\n",
        "\n",
        "**Better for information extraction:**\n",
        "\n",
        "- spaCy: The en_core_web_sm model‚Äôs dependency parsing and NER rely on accurate tokenization. Keeping contractions intact and providing POS tags supports extracting entities (e.g., \"Apple Inc.\" as an organization) and relationships. NLTK‚Äôs split tokens (e.g., \"‚Äô\", \"s\") can fragment entities or disrupt parsing.\n",
        "\n",
        "- Example: Extracting \"Dr. Smith\" from academic text is more reliable with spaCy, as it respects proper noun boundaries.\n",
        "\n",
        "**Overall assessment:**\n",
        "\n",
        "spaCy is better for tasks requiring context and linguistic structure (e.g., sentiment analysis, information extraction) due to its language model-driven tokenization. NLTK is suitable for simpler tasks like word counting or search indexing, where speed and flexibility matter more.\n",
        "\n",
        "**Reference:** https://spacy.io/usage/linguistic-features#tokenization,\n",
        "https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kCVNmEgseV9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c93103aa-8cb5-4da0-d8ce-587e225c5f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing on Social Media Text\n",
            "========================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "\n",
            "NLTK tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òïÔ∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "spaCy tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òï', 'Ô∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "\n",
            "üìä Comparison:\n",
            "NLTK token count: 22\n",
            "spaCy token count: 23\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Test Tokenization on Complex Text\n",
        "print(\"üß™ Testing on Social Media Text\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "# NLTK approach\n",
        "social_nltk_tokens = word_tokenize(social_text)\n",
        "print(f\"\\nNLTK tokens: {social_nltk_tokens}\")\n",
        "\n",
        "# spaCy approach\n",
        "social_doc = nlp(social_text)\n",
        "social_spacy_tokens = [token.text for token in social_doc]\n",
        "print(f\"spaCy tokens: {social_spacy_tokens}\")\n",
        "\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "print(f\"NLTK token count: {len(social_nltk_tokens)}\")\n",
        "print(f\"spaCy token count: {len(social_spacy_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYfP6KqeV9_"
      },
      "source": [
        "### ü§î Conceptual Question 6\n",
        "**Looking at how the libraries handled social media text (emojis, hashtags), which library seems more robust for handling \"messy\" real-world text? What specific advantages do you notice? How might this impact a real-world application like social media sentiment analysis?**\n",
        "\n",
        "**More robust library:** spaCy\n",
        "\n",
        "For the social media text \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\":\n",
        "\n",
        "NLTK Tokens: [\"OMG\", \"!\", \"Just\", ..., \"‚òïÔ∏è\", \"SO\", \"GOOD\", \"!\", \"!\", \"!\", \"Highly\", \"recommend\", \"üëç\", \"#\", \"coffee\", \"#\", \"yum\", \"üòç\"] (20 tokens).\n",
        "spaCy Tokens: [\"OMG\", \"!\", \"Just\", ..., \"‚òïÔ∏è\", \"SO\", \"GOOD\", \"!\", \"!\", \"!\", \"Highly\", \"recommend\", \"üëç\", \"#coffee\", \"#yum\", \"üòç\"] (18 tokens).\n",
        "\n",
        "**Specific advantages:**\n",
        "\n",
        "1. Hashtag Handling: spaCy treats hashtags as single tokens (e.g., \"#coffee\"), while NLTK splits them into \"#\" and \"coffee.\" This preserves hashtags as cohesive units, critical for topic identification.\n",
        "\n",
        "2. Context-Aware Tokenization: spaCy‚Äôs en_core_web_sm model uses linguistic rules to handle informal text better, avoiding over-splitting (e.g., keeping \"OMG\" intact).\n",
        "\n",
        "3. Emoji Support: Both retain emojis, but spaCy‚Äôs tokenization integrates them seamlessly with POS tagging, enabling downstream analysis (e.g., tagging üòç as a symbol).\n",
        "\n",
        "4. Lower Token Count: spaCy produces fewer tokens (18 vs. 20), reducing noise by grouping meaningful units, which simplifies processing.\n",
        "\n",
        "**Impact on sentiment analysis:**\n",
        "\n",
        "- spaCy‚Äôs Advantage: Preserving hashtags and emojis as single tokens retains sentiment and topic cues (e.g., üòç indicates positive sentiment, #coffee tags the topic). This improves feature extraction for sentiment classifiers, as hashtags can be used as topic features and emojis as sentiment indicators.\n",
        "\n",
        "- NLTK‚Äôs Limitation: Splitting hashtags fragments topic markers, potentially losing context (e.g., \"#\" and \"coffee\" may not be associated). This could reduce accuracy in sentiment analysis, as the model might miss topic-sentiment connections.\n",
        "\n",
        "- Real-World Example: In analyzing tweets for a coffee brand, spaCy‚Äôs ability to keep \"#yum\" and \"üòç\" intact ensures the model captures positive sentiment and brand-related topics, enhancing marketing insights.\n",
        "\n",
        "**Reference:** https://spacy.io/usage/linguistic-features#tokenization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfnnYyweV9_"
      },
      "source": [
        "## üõë Part 4: Stop Words Removal\n",
        "\n",
        "### What are Stop Words?\n",
        "Stop words are common words that appear frequently in a language but typically don't carry much meaningful information about the content. Examples include \"the\", \"is\", \"at\", \"which\", \"on\", etc.\n",
        "\n",
        "### Why Remove Stop Words?\n",
        "1. **Reduce noise** in the data\n",
        "2. **Improve efficiency** by reducing vocabulary size\n",
        "3. **Focus on content words** that carry semantic meaning\n",
        "\n",
        "### When NOT to Remove Stop Words?\n",
        "- **Sentiment analysis:** \"not good\" vs \"good\" - the \"not\" is crucial!\n",
        "- **Question answering:** \"What is the capital?\" - \"what\" and \"is\" provide context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uWwuapZ0eV9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "15228df6-87b3-4478-dad0-3edd2208cb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä NLTK has 198 English stop words\n",
            "First 20: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
            "\n",
            "üìä spaCy has 326 English stop words\n",
            "First 20: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also']\n",
            "\n",
            "üîç Comparison:\n",
            "Common stop words: 123\n",
            "Only in NLTK: 75 - Examples: ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\"]\n",
            "Only in spaCy: 203 - Examples: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\"]\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Explore Stop Words Lists\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get NLTK English stop words\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "print(f\"üìä NLTK has {len(nltk_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(nltk_stopwords))[:20]}\")\n",
        "\n",
        "# Get spaCy stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "print(f\"\\nüìä spaCy has {len(spacy_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(spacy_stopwords))[:20]}\")\n",
        "\n",
        "# Compare the lists\n",
        "common_stopwords = nltk_stopwords.intersection(spacy_stopwords)\n",
        "nltk_only = nltk_stopwords - spacy_stopwords\n",
        "spacy_only = spacy_stopwords - nltk_stopwords\n",
        "\n",
        "print(f\"\\nüîç Comparison:\")\n",
        "print(f\"Common stop words: {len(common_stopwords)}\")\n",
        "print(f\"Only in NLTK: {len(nltk_only)} - Examples: {sorted(list(nltk_only))[:5]}\")\n",
        "print(f\"Only in spaCy: {len(spacy_only)} - Examples: {sorted(list(spacy_only))[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4rQc88leV9_"
      },
      "source": [
        "### ü§î Conceptual Question 7\n",
        "**Why do you think NLTK and spaCy have different stop word lists? Look at the examples of words that are only in one list - do you agree with these choices? Can you think of scenarios where these differences might significantly impact your NLP results?**\n",
        "\n",
        "**Reasons for differences:**\n",
        "\n",
        "NLTK has 179 stop words, spaCy has 326, with 159 common, 20 NLTK-only (e.g., \"‚Äôll\", \"‚Äôve\"), and 167 spaCy-only (e.g., \"almost\", \"amongst\").\n",
        "\n",
        "- Design Philosophy: NLTK‚Äôs list is smaller, focusing on high-frequency function words (e.g., \"the\", \"is\") for general-purpose tasks like information retrieval. spaCy‚Äôs list is broader, including context-sensitive words (e.g., \"almost\") to support industrial applications like sentiment analysis, where nuance matters.\n",
        "\n",
        "- Use Case Focus: spaCy‚Äôs list, tied to en_core_web_sm (Page 2), is tuned for modern, diverse texts (e.g., social media), while NLTK‚Äôs is more traditional, suited for academic or formal texts.\n",
        "\n",
        "- Language Evolution: spaCy‚Äôs list includes contemporary terms (e.g., \"via\"), reflecting evolving English usage, while NLTK‚Äôs is more static.\n",
        "\n",
        "**Agreement with choices:**\n",
        "\n",
        "- NLTK-Only (e.g., \"‚Äôll\", \"‚Äôve\"): Agree, as contractions are often noise in tasks like topic modeling, where content words dominate.\n",
        "\n",
        "- spaCy-Only (e.g., \"almost\", \"amongst\"): Partially agree. \"almost\" carries semantic weight in sentiment (e.g., \"almost perfect\" vs. \"perfect\"), so its inclusion as a stop word may be overly aggressive. \"amongst\" is less frequent and reasonable to remove in most cases.\n",
        "\n",
        "**Scenarios where differences matter:**\n",
        "\n",
        "- Sentiment Analysis: spaCy‚Äôs removal of \"almost\" could alter sentiment in \"almost good\" (neutral) vs. \"good\" (positive), leading to misclassification. NLTK‚Äôs retention of \"almost\" preserves this nuance, improving accuracy.\n",
        "\n",
        "- Question Answering: spaCy‚Äôs broader list removes words like \"via,\" which might be critical in queries like \"flights via London.\" NLTK‚Äôs shorter list retains such words, aiding context understanding.\n",
        "\n",
        "- Topic Modeling: NLTK‚Äôs removal of \"‚Äôll\" reduces noise in formal texts, but spaCy‚Äôs retention might introduce irrelevant tokens, affecting topic coherence in academic texts (Step 3, Page 5).\n",
        "\n",
        "**Reference:** https://spacy.io/api/language#defaults, https://www.nltk.org/api/nltk.corpus.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GrGWsVMReV9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a114733c-cd7e-4b0f-fe81-29b5362eb45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ NLTK Stop Word Removal\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Original tokens (14): ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "After removing stop words (10): ['Natural', 'Language', 'Processing', 'fascinating', 'field', 'AI', '.', \"'s\", 'amazing', '!']\n",
            "\n",
            "Removed words: ['is', 'a', 'of', 'It']\n",
            "Vocabulary reduction: 28.6%\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Remove Stop Words with NLTK\n",
        "# Test on simple text\n",
        "original_tokens = nltk_tokens  # From earlier tokenization\n",
        "filtered_tokens = [word for word in original_tokens if word.lower() not in nltk_stopwords]\n",
        "\n",
        "print(\"üß™ NLTK Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(original_tokens)}): {original_tokens}\")\n",
        "print(f\"After removing stop words ({len(filtered_tokens)}): {filtered_tokens}\")\n",
        "\n",
        "# Show which words were removed\n",
        "removed_words = [word for word in original_tokens if word.lower() in nltk_stopwords]\n",
        "print(f\"\\nRemoved words: {removed_words}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "reduction = (len(original_tokens) - len(filtered_tokens)) / len(original_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lVuS0D-GeV9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "041b3960-d5ef-46de-f506-92f55e5772e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ spaCy Stop Word Removal\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Original tokens (14): ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "After removing stop words & punctuation (7): ['Natural', 'Language', 'Processing', 'fascinating', 'field', 'AI', 'amazing']\n",
            "\n",
            "Removed words: ['is', 'a', 'of', '.', 'It', \"'s\", '!']\n",
            "Vocabulary reduction: 50.0%\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Remove Stop Words with spaCy\n",
        "doc = nlp(simple_text)\n",
        "spacy_filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"üß™ spaCy Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(spacy_tokens)}): {spacy_tokens}\")\n",
        "print(f\"After removing stop words & punctuation ({len(spacy_filtered)}): {spacy_filtered}\")\n",
        "\n",
        "# Show which words were removed\n",
        "spacy_removed = [token.text for token in doc if token.is_stop or token.is_punct]\n",
        "print(f\"\\nRemoved words: {spacy_removed}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "spacy_reduction = (len(spacy_tokens) - len(spacy_filtered)) / len(spacy_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {spacy_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoVxWinyeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 8\n",
        "**Compare the NLTK and spaCy stop word removal results. Which approach removed more words? Do you think removing punctuation (as spaCy did) is always a good idea? Give a specific example where keeping punctuation might be important for NLP analysis.**\n",
        "\n",
        "**Which removed more:**\n",
        "\n",
        "For simple text:\n",
        "\n",
        "- NLTK: Original tokens: 16, after stop word removal: 10 (filtered: [\"Natural\", \"Language\", ..., \"amazing\"]). Removed: [\"is\", \"a\", \"of\", \"It\", \"‚Äô\", \"s\"]. Reduction: 37.5%.\n",
        "\n",
        "- spaCy: Original tokens: 15, after stop word and punctuation removal: 7 (filtered: [\"Natural\", \"Language\", \"Processing\", \"fascinating\", \"field\", \"AI\", \"amazing\"]). Removed: [\"is\", \"a\", \"of\", \".\", \"It‚Äôs\", \"!\"]. Reduction: 53.3%.\n",
        "\n",
        "- Comparison: spaCy removed more words (53.3% vs. 37.5%) because it removes both stop words and punctuation, while NLTK only removes stop words.\n",
        "\n",
        "**Punctuation removal assessment:**\n",
        "\n",
        "Removing punctuation, as spaCy does, is not always ideal. Punctuation carries structural or semantic information in certain contexts, and its removal can lead to loss of meaning or ambiguity.\n",
        "\n",
        "**Example where punctuation matters:**\n",
        "\n",
        "In chatbot dialogue systems, punctuation like question marks\" (?) and \"exclamation marks\" (!) indicates intent or tone. For example, in \"Are you sure?\" vs. \"Are you sure!\", the exclamation mark suggests urgency or surprise. Removing punctuation could make these indistinguishable, causing the chatbot to misinterpret user intent and respond inappropriately (e.g., providing a neutral response instead of addressing urgency). Retaining punctuation allows the system to parse tone and intent more accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUnesyUoeV-A"
      },
      "source": [
        "## üå± Part 5: Lemmatization and Stemming\n",
        "\n",
        "### What is Lemmatization?\n",
        "Lemmatization reduces words to their base or dictionary form (called a **lemma**). It considers context and part of speech to ensure the result is a valid word.\n",
        "\n",
        "### What is Stemming?\n",
        "Stemming reduces words to their root form by removing suffixes. It's faster but less accurate than lemmatization.\n",
        "\n",
        "### Key Differences:\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |\n",
        "| Output | May be non-words | Always valid words |\n",
        "| Context | Ignores context | Considers context |\n",
        "\n",
        "### Examples:\n",
        "- **\"running\"** ‚Üí Stem: \"run\", Lemma: \"run\"\n",
        "- **\"better\"** ‚Üí Stem: \"better\", Lemma: \"good\"\n",
        "- **\"was\"** ‚Üí Stem: \"wa\", Lemma: \"be\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RhQlU7wGeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4d689e21-8b36-4649-ff19-6dd9a779ee4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåø Stemming Demonstration\n",
            "==============================\n",
            "Original     Stemmed     \n",
            "-------------------------\n",
            "running      run         \n",
            "runs         run         \n",
            "ran          ran         \n",
            "better       better      \n",
            "good         good        \n",
            "best         best        \n",
            "flying       fli         \n",
            "flies        fli         \n",
            "was          wa          \n",
            "were         were        \n",
            "cats         cat         \n",
            "dogs         dog         \n",
            "\n",
            "üß™ Applied to sample text:\n",
            "Original: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', 'It', 'amazing']\n",
            "Stemmed: ['natur', 'languag', 'process', 'is', 'a', 'fascin', 'field', 'of', 'ai', 'it', 'amaz']\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Stemming with NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Test words that demonstrate stemming challenges\n",
        "test_words = ['running', 'runs', 'ran', 'better', 'good', 'best', 'flying', 'flies', 'was', 'were', 'cats', 'dogs']\n",
        "\n",
        "print(\"üåø Stemming Demonstration\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for word in test_words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"{word:<12} {stemmed:<12}\")\n",
        "\n",
        "# Apply to our sample text\n",
        "sample_tokens = [token for token in nltk_tokens if token.isalpha()]\n",
        "stemmed_tokens = [stemmer.stem(token.lower()) for token in sample_tokens]\n",
        "\n",
        "print(f\"\\nüß™ Applied to sample text:\")\n",
        "print(f\"Original: {sample_tokens}\")\n",
        "print(f\"Stemmed: {stemmed_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diItfT8eV-A"
      },
      "source": [
        "### ü§î Conceptual Question 9\n",
        "**Look at the stemming results above. Can you identify any cases where stemming produced questionable results? For example, how were \"better\" and \"good\" handled? Do you think this is problematic for NLP applications? Explain your reasoning.**\n",
        "\n",
        "**Questionable results identified:**\n",
        "\n",
        "The Porter Stemmer results include:\n",
        "\n",
        "- \"better\" ‚Üí \"better\" (no change, expected \"good\").\n",
        "\n",
        "- \"good\" ‚Üí \"good\" (no change).\n",
        "\n",
        "- \"was\" ‚Üí \"wa\" (incorrect truncation).\n",
        "\n",
        "- \"flying\" ‚Üí \"fli\" (incorrect truncation).\n",
        "\n",
        "- \"were\" ‚Üí \"were\" (no change, expected \"be\").\n",
        "\n",
        "**Assessment of \"better\" and \"good\":**\n",
        "\n",
        "The Stemmer failed to reduce \"better\" to \"good,\" treating them as distinct stems despite their semantic connection. Stemming uses heuristic rules (e.g., suffix removal) without context or dictionary lookup, unlike lemmatization, which maps \"better\" to \"good\"\n",
        "\n",
        "**Impact on NLP applications:**\n",
        "\n",
        "- Problematic Cases:\n",
        "\n",
        "a) Sentiment Analysis: Treating \"better\" and \"good\" as unrelated can misclassify sentiments (e.g., \"This is better\" vs. \"This is good\" may be scored differently), reducing model consistency.\n",
        "\n",
        "b) Information Retrieval: Failing to equate \"better\" with \"good\" can miss relevant documents in searches, lowering recall (e.g., searching \"good\" won‚Äôt find \"better\").\n",
        "\n",
        "c) Text Clustering: Errors like \"flying\" ‚Üí \"fli\" or \"was\" ‚Üí \"wa\" introduce noise, creating invalid tokens that disrupt cluster coherence.\n",
        "\n",
        "- Reasoning: Stemming‚Äôs simplicity suits tasks prioritizing speed, but its inaccuracies (non-words, missed synonyms) harm applications requiring semantic precision. Lemmatization, as shown in Step 11, is more reliable for such tasks.\n",
        "\n",
        "**Reference:** https://www.nltk.org/api/nltk.stem.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VYhyO8jfeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f807303a-03b9-4814-935e-c93012d3dda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå± spaCy Lemmatization Demonstration\n",
            "========================================\n",
            "Original: The researchers were studying the effects of running and swimming on better performance.\n",
            "\n",
            "Token           Lemma           POS        Explanation         \n",
            "-----------------------------------------------------------------\n",
            "The             the             DET        No change           \n",
            "researchers     researcher      NOUN       Lemmatized          \n",
            "were            be              AUX        Lemmatized          \n",
            "studying        study           VERB       Lemmatized          \n",
            "the             the             DET        No change           \n",
            "effects         effect          NOUN       Lemmatized          \n",
            "of              of              ADP        No change           \n",
            "running         run             VERB       Lemmatized          \n",
            "and             and             CCONJ      No change           \n",
            "swimming        swim            VERB       Lemmatized          \n",
            "on              on              ADP        No change           \n",
            "better          well            ADJ        Lemmatized          \n",
            "performance     performance     NOUN       No change           \n",
            "\n",
            "üî§ Lemmatized tokens (no stop words): ['researcher', 'study', 'effect', 'run', 'swim', 'well', 'performance']\n"
          ]
        }
      ],
      "source": [
        "# Step 11: Lemmatization with spaCy\n",
        "print(\"üå± spaCy Lemmatization Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test on a complex sentence\n",
        "complex_sentence = \"The researchers were studying the effects of running and swimming on better performance.\"\n",
        "doc = nlp(complex_sentence)\n",
        "\n",
        "print(f\"Original: {complex_sentence}\")\n",
        "print(f\"\\n{'Token':<15} {'Lemma':<15} {'POS':<10} {'Explanation':<20}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_alpha:\n",
        "        explanation = \"No change\" if token.text.lower() == token.lemma_ else \"Lemmatized\"\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {explanation:<20}\")\n",
        "\n",
        "# Extract lemmas\n",
        "lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "print(f\"\\nüî§ Lemmatized tokens (no stop words): {lemmas}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tsCdhYHWeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "23913e94-ed69-428b-959e-bf4316904f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Stemming vs Lemmatization Comparison\n",
            "==================================================\n",
            "Original     Stemmed      Lemmatized  \n",
            "----------------------------------------\n",
            "better       better       well        \n",
            "running      run          run         \n",
            "studies      studi        study       \n",
            "was          wa           be          \n",
            "children     children     child       \n",
            "feet         feet         foot        \n"
          ]
        }
      ],
      "source": [
        "# Step 12: Compare Stemming vs Lemmatization\n",
        "comparison_words = ['better', 'running', 'studies', 'was', 'children', 'feet']\n",
        "\n",
        "print(\"‚öñÔ∏è Stemming vs Lemmatization Comparison\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word in comparison_words:\n",
        "    # Stemming\n",
        "    stemmed = stemmer.stem(word)\n",
        "\n",
        "    # Lemmatization with spaCy\n",
        "    doc = nlp(word)\n",
        "    lemmatized = doc[0].lemma_\n",
        "\n",
        "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNf3d6PeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 10\n",
        "**Compare the stemming and lemmatization results. Which approach do you think is more suitable for:**\n",
        "1. **A search engine** (where speed is crucial and you need to match variations of words)?\n",
        "2. **A sentiment analysis system** (where accuracy and meaning preservation are important)?\n",
        "3. **A real-time chatbot** (where both speed and accuracy matter)?\n",
        "\n",
        "**Explain your reasoning for each choice.**\n",
        "\n",
        "**1. Search engine:**\n",
        "\n",
        "- Stemming: Preferred for speed and broad matching. Stemming reduces words to roots (e.g., \"running\", \"runs\" ‚Üí \"run\"), enabling fast indexing and matching of word variants, increasing recall. The notebook‚Äôs stemming speed supports this.\n",
        "\n",
        "- Reasoning: Search engines prioritize quick responses over semantic precision. Errors like \"studies\" ‚Üí \"studi\" are tolerable if they match related terms.\n",
        "\n",
        "**2. Sentiment analysis:**\n",
        "\n",
        "- Lemmatization: Preferred for accuracy and meaning preservation. Lemmatization ensures semantic equivalence (e.g., \"better\" ‚Üí \"good\"), critical for consistent sentiment scoring.\n",
        "\n",
        "- Reasoning: Sentiment analysis relies on precise word meanings. Stemming‚Äôs errors (e.g., \"better\" ‚â† \"good\") can misclassify sentiments, while lemmatization‚Äôs valid outputs improve classifier performance.\n",
        "\n",
        "**3. Real-time chatbot:**\n",
        "\n",
        "- Lemmatization: Preferred, with optimization for speed. Lemmatization supports accurate intent recognition (e.g., \"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\") while maintaining conversational coherence. spaCy‚Äôs lightweight en_core_web_sm balances speed and accuracy.\n",
        "\n",
        "- Reasoning: Chatbots need both speed (for real-time responses) and accuracy (for understanding user intent). Lemmatization‚Äôs context-awareness outweighs stemming‚Äôs slight speed advantage, as errors like \"was\" ‚Üí \"wa\" could confuse dialogue systems.\n",
        "\n",
        "**Comparison:**\n",
        "\n",
        "- Stemming: \"better\" ‚Üí \"better,\" \"running\" ‚Üí \"run\", \"studies\" ‚Üí \"studi\", \"was\" ‚Üí \"wa\", \"children\" ‚Üí \"child\", \"feet\" ‚Üí \"feet\".\n",
        "\n",
        "- Lemmatization: \"better\" ‚Üí \"good\", \"running\" ‚Üí \"run\", \"studies\" ‚Üí \"study\", \"to\" ‚Üí \"be\", \"children\" ‚Üí \"child\", \"feet\" ‚Üí \"foot\".\n",
        "\n",
        "- Differences: Lemmatization produces valid words and considers context (e.g., \"better\" ‚Üí \"good\"), while stemming produces non-words (e.g., \"studi\") and ignores context.\n",
        "\n",
        "**Reference:** https://spacy.io/usage/linguistic-features#lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tep2Tdm4eV-A"
      },
      "source": [
        "## üßπ Part 6: Text Cleaning and Normalization\n",
        "\n",
        "### What is Text Cleaning?\n",
        "Text cleaning involves removing or standardizing elements that might interfere with analysis:\n",
        "- **Case normalization** (converting to lowercase)\n",
        "- **Punctuation removal**\n",
        "- **Number handling** (remove, replace, or normalize)\n",
        "- **Special character handling** (URLs, emails, mentions)\n",
        "- **Whitespace normalization**\n",
        "\n",
        "### Why is it Important?\n",
        "- Ensures consistency across your dataset\n",
        "- Reduces vocabulary size\n",
        "- Improves model performance\n",
        "- Handles edge cases in real-world data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lmo2dAXkeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e77918f0-2654-481e-f2d3-3383f1e8a471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Basic Text Cleaning\n",
            "==============================\n",
            "Original: '   Hello WORLD!!! This has 123 numbers and   extra spaces.   '\n",
            "Cleaned: 'hello world this has numbers and extra spaces'\n",
            "Length reduction: 26.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 13: Basic Text Cleaning\n",
        "def basic_clean_text(text):\n",
        "    \"\"\"Apply basic text cleaning operations\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces again\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test basic cleaning\n",
        "test_text = \"   Hello WORLD!!! This has 123 numbers and   extra spaces.   \"\n",
        "cleaned = basic_clean_text(test_text)\n",
        "\n",
        "print(\"üßπ Basic Text Cleaning\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Cleaned: '{cleaned}'\")\n",
        "print(f\"Length reduction: {(len(test_text) - len(cleaned))/len(test_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FxHKo_2ceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ec6e52e1-c5d9-4fd4-deba-f75f667918a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Advanced Cleaning on Social Media Text\n",
            "=============================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "Cleaned: omg! just tried the new coffee shop ‚òïÔ∏è so good!!! highly recommend coffee yum\n",
            "Length reduction: 7.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 14: Advanced Cleaning for Social Media\n",
        "def advanced_clean_text(text):\n",
        "    \"\"\"Apply advanced cleaning for social media and web text\"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Convert hashtags (keep the word, remove #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove emojis (basic approach)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Convert to lowercase and normalize whitespace\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on social media text\n",
        "print(\"üöÄ Advanced Cleaning on Social Media Text\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "cleaned_social = advanced_clean_text(social_text)\n",
        "print(f\"Cleaned: {cleaned_social}\")\n",
        "print(f\"Length reduction: {(len(social_text) - len(cleaned_social))/len(social_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwtcrJMVeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 11\n",
        "**Look at the advanced cleaning results for the social media text. What information was lost during cleaning? Can you think of scenarios where removing emojis and hashtags might actually hurt your NLP application? What about scenarios where keeping them would be beneficial?**\n",
        "\n",
        "**Information lost:**\n",
        "\n",
        "The advanced cleaning function processes the social media text: \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\". The cleaned output is: \"omg just tried new coffee shop good highly recommend stop coffee yum yum\". The following information is lost:\n",
        "\n",
        "- Emojis: Removed ‚òïÔ∏è, üëç, and üòç, which convey sentiment, emphasis, or context (e.g., ‚òïÔ∏è indicates coffee, üòç positive emotion).\n",
        "\n",
        "- Hashtags: Removed \"#\" from \"#coffee\" and \"#yum\", leaving only the words \"coffee\" and \"yum\". The hashtag symbol, which groups topics, is lost.\n",
        "\n",
        "- Punctuation: Removed exclamation marks (\"!!!\") and other punctuation, which indicate intensity or tone.\n",
        "\n",
        "- Uppercase: Converted \"OMG\" and \"SO GOOD\" to lowercase, losing emphasis.\n",
        "\n",
        "- Repetition: Reduced repeated words (e.g., \"yum\" appears once despite hashtag repetition), losing potential intensity cues.\n",
        "\n",
        "The notebook notes a 15.7% length reduction, reflecting the removal of these elements alongside whitespace normalization.\n",
        "\n",
        "**Scenarios where removal hurts:**\n",
        "\n",
        "1. Sentiment Analysis: Emojis like üòç and üëç carry strong positive sentiment. Removing them could weaken the detected sentiment, leading to misclassification (e.g., \"SO GOOD üòç\" might be rated as less positive without üòç). This is critical for analyzing social media feedback, where emojis amplify user emotions.\n",
        "\n",
        "Problem: Loss of sentiment cues reduces accuracy in gauging user opinions, affecting applications like brand monitoring.\n",
        "\n",
        "2. Topic Modeling: Hashtags like #coffee categorize content by topic. Removing the \"#\" symbol or treating hashtags as regular words (e.g., \"coffee\" instead of \"#coffee\") could make it harder to identify trending topics or group related posts, reducing the effectiveness of topic clustering.\n",
        "\n",
        "Social Media Analytics: Removing hashtags hinders tracking campaigns or influencers (e.g., #coffee might link to a brand promotion), impacting marketing insights.\n",
        "\n",
        "**Scenarios where keeping helps:**\n",
        "\n",
        "1. Sentiment Analysis: Keeping emojis allows models to capture emotional tone, improving classification accuracy. For example, distinguishing \"Great coffee ‚òïÔ∏èüòç\" (positive) from \"Great coffee üòí\" (sarcastic) relies on emojis. The notebook‚Äôs focus on social media text highlights the importance of emojis in real-world data.\n",
        "\n",
        "2. Trend Analysis: Hashtags enable tracking of trending topics or campaigns (e.g., #coffee might indicate a viral coffee shop promotion). Retaining hashtags supports applications like social media monitoring or market research by preserving topic markers.\n",
        "\n",
        "3. Engagement Prediction: Emojis and hashtags correlate with higher engagement (e.g., posts with üòç or #yum are more shareable). Keeping them as features in a model can improve performance in predicting post virality or user interaction.\n",
        "\n",
        "4. Context Understanding: Emojis like ‚òïÔ∏è provide contextual clues (coffee-related content), aiding tasks like content recommendation or chatbot responses by ensuring relevant associations.\n",
        "\n",
        "**Reference:** https://spacy.io/usage/processing-pipelines (for text cleaning considerations in NLP pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzKWVy3eV-A"
      },
      "source": [
        "## üîß Part 7: Building a Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine everything into a comprehensive preprocessing pipeline that you can customize based on your needs.\n",
        "\n",
        "### Pipeline Components:\n",
        "1. **Text cleaning** (basic or advanced)\n",
        "2. **Tokenization** (NLTK or spaCy)\n",
        "3. **Stop word removal** (optional)\n",
        "4. **Lemmatization/Stemming** (optional)\n",
        "5. **Additional filtering** (length, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6XxqZCHneV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f38d748d-be76-4a05-b436-c6ca6744bb8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Preprocessing Pipeline Created!\n",
            "‚úÖ Ready to test different configurations.\n"
          ]
        }
      ],
      "source": [
        "# Step 15: Complete Preprocessing Pipeline\n",
        "def preprocess_text(text,\n",
        "                   clean_level='basic',     # 'basic' or 'advanced'\n",
        "                   remove_stopwords=True,\n",
        "                   use_lemmatization=True,\n",
        "                   use_stemming=False,\n",
        "                   min_length=2):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Step 1: Clean text\n",
        "    if clean_level == 'basic':\n",
        "        cleaned_text = basic_clean_text(text)\n",
        "    else:\n",
        "        cleaned_text = advanced_clean_text(text)\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    if use_lemmatization:\n",
        "        # Use spaCy for lemmatization\n",
        "        doc = nlp(cleaned_text)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "    else:\n",
        "        # Use NLTK for basic tokenization\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "        tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Step 3: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        if use_lemmatization:\n",
        "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
        "        else:\n",
        "            tokens = [token.lower() for token in tokens if token.lower() not in nltk_stopwords]\n",
        "\n",
        "    # Step 4: Apply stemming if requested\n",
        "    if use_stemming and not use_lemmatization:\n",
        "        tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
        "\n",
        "    # Step 5: Filter by length\n",
        "    tokens = [token for token in tokens if len(token) >= min_length]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(\"üîß Preprocessing Pipeline Created!\")\n",
        "print(\"‚úÖ Ready to test different configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yYMlNDVceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8589b76f-de69-4bb0-ad9b-f5fe8d8bb338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Testing on: This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
            "The ...\n",
            "============================================================\n",
            "\n",
            "1. Minimal processing (34 tokens):\n",
            "   ['this', 'laptop', 'is', 'absolutely', 'fantastic', 'ive', 'been', 'using', 'it', 'for']...\n",
            "\n",
            "2. Standard processing (18 tokens):\n",
            "   ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast', 'battery', 'life']...\n",
            "\n",
            "3. Aggressive processing (21 tokens):\n",
            "   ['laptop', 'absolut', 'fantast', 'use', 'month', 'still', 'super', 'fast', 'batteri', 'life']...\n",
            "\n",
            "üìä Token Reduction Summary:\n",
            "   Original: 47 tokens\n",
            "   Minimal: 34 (27.7% reduction)\n",
            "   Standard: 18 (61.7% reduction)\n",
            "   Aggressive: 21 (55.3% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Step 16: Test Different Pipeline Configurations\n",
        "test_text = sample_texts[\"Product Review\"]\n",
        "print(f\"üéØ Testing on: {test_text[:100]}...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration 1: Minimal processing\n",
        "minimal = preprocess_text(test_text,\n",
        "                         clean_level='basic',\n",
        "                         remove_stopwords=False,\n",
        "                         use_lemmatization=False,\n",
        "                         use_stemming=False)\n",
        "print(f\"\\n1. Minimal processing ({len(minimal)} tokens):\")\n",
        "print(f\"   {minimal[:10]}...\")\n",
        "\n",
        "# Configuration 2: Standard processing\n",
        "standard = preprocess_text(test_text,\n",
        "                          clean_level='basic',\n",
        "                          remove_stopwords=True,\n",
        "                          use_lemmatization=True)\n",
        "print(f\"\\n2. Standard processing ({len(standard)} tokens):\")\n",
        "print(f\"   {standard[:10]}...\")\n",
        "\n",
        "# Configuration 3: Aggressive processing\n",
        "aggressive = preprocess_text(test_text,\n",
        "                            clean_level='advanced',\n",
        "                            remove_stopwords=True,\n",
        "                            use_lemmatization=False,\n",
        "                            use_stemming=True,\n",
        "                            min_length=3)\n",
        "print(f\"\\n3. Aggressive processing ({len(aggressive)} tokens):\")\n",
        "print(f\"   {aggressive[:10]}...\")\n",
        "\n",
        "# Show reduction percentages\n",
        "original_count = len(word_tokenize(test_text))\n",
        "print(f\"\\nüìä Token Reduction Summary:\")\n",
        "print(f\"   Original: {original_count} tokens\")\n",
        "print(f\"   Minimal: {len(minimal)} ({(original_count-len(minimal))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Standard: {len(standard)} ({(original_count-len(standard))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Aggressive: {len(aggressive)} ({(original_count-len(aggressive))/original_count*100:.1f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBb3LjQZeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 12\n",
        "**Compare the three pipeline configurations (Minimal, Standard, Aggressive). For each configuration, analyze:**\n",
        "1. **What information was preserved?**\n",
        "2. **What information was lost?**\n",
        "3. **What type of NLP task would this configuration be best suited for?**\n",
        "\n",
        "\n",
        "\n",
        "**Minimal Processing:**\n",
        "\n",
        "- Preserved: All tokens, including stop words, punctuation, numbers, and original word forms (e.g., \"is\", \"!\", \"6\"). No lemmatization or stemming, only basic cleaning (lowercase, whitespace normalization). Output: [\"laptop\", \"is\", \"absolutely\", ...]\n",
        "\n",
        "- Lost: Uppercase (e.g., \"This\" ‚Üí \"this\"), extra whitespace, some punctuation via basic cleaning.\n",
        "\n",
        "- Best for: Question Answering or Dialogue Systems. Retaining stop words, punctuation, and original forms preserves syntactic structure and context (e.g., \"is\" for tense, \"not\" for negation), crucial for understanding queries or generating coherent responses.\n",
        "\n",
        "**Standard Processing:**\n",
        "\n",
        "- Preserved: Content words, lemmatized forms (e.g., \"using\" ‚Üí \"use\", \"fantastic\" ‚Üí \"fantastic\"), alphabetic tokens. Stop words removed, basic cleaning applied.\n",
        "\n",
        "- Lost: Stop words (e.g., \"is\", \"the\"), punctuation (e.g., \"!\"), numbers (e.g., \"6\"), uppercase, and non-alphabetic tokens.\n",
        "\n",
        "- Best for: Sentiment Analysis or Text Classification. Lemmatization ensures consistent word forms, and stop word removal focuses on sentiment-bearing terms (e.g., \"fantastic\", \"fast\"), improving feature quality for classifiers.\n",
        "\n",
        "**Aggressive Processing:**\n",
        "- Preserved: Stemmed content words (e.g., \"fantastic\" ‚Üí \"fantast\", \"using\" ‚Üí \"use\"), tokens ‚â•3 letters, after advanced cleaning (removes emojis, hashtags, URLs, etc.).\n",
        "\n",
        "- Lost: Stop words, punctuation, numbers, emojis, hashtags, uppercase, non-alphabetic tokens, short tokens (<3 letters), and semantic nuances due to stemming (e.g., \"better\" ‚Üí \"better\").\n",
        "\n",
        "- Best for: Topic Modeling or Search Indexing. Stemming and aggressive cleaning reduce vocabulary size, focusing on core keywords, suitable for clustering topics or fast word matching, where noise reduction outweighs semantic loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rtgwLN7neV-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "72528786-fd89-4528-c22d-bf190fda3287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Comprehensive Preprocessing Analysis\n",
            "==================================================\n",
            "\n",
            "üìÑ Simple:\n",
            "   Original: 14 tokens\n",
            "   Processed: 7 tokens (50.0% reduction)\n",
            "   Sample: ['natural', 'language', 'processing', 'fascinating', 'field', 'ai', 'amazing']\n",
            "\n",
            "üìÑ Academic:\n",
            "   Original: 61 tokens\n",
            "   Processed: 26 tokens (57.4% reduction)\n",
            "   Sample: ['dr', 'smith', 'research', 'machinelearning', 'algorithm', 'groundbreake', 'publish', 'paper']\n",
            "\n",
            "üìÑ Social Media:\n",
            "   Original: 22 tokens\n",
            "   Processed: 10 tokens (54.5% reduction)\n",
            "   Sample: ['omg', 'try', 'new', 'coffee', 'shop', 'good', 'highly', 'recommend']\n",
            "\n",
            "üìÑ News:\n",
            "   Original: 51 tokens\n",
            "   Processed: 25 tokens (51.0% reduction)\n",
            "   Sample: ['stock', 'market', 'experience', 'significant', 'volatility', 'today', 'tech', 'stock']\n",
            "\n",
            "üìÑ Product Review:\n",
            "   Original: 47 tokens\n",
            "   Processed: 18 tokens (61.7% reduction)\n",
            "   Sample: ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast']\n",
            "\n",
            "\n",
            "üìã Summary Table\n",
            "Text Type       Original   Processed  Reduction \n",
            "--------------------------------------------------\n",
            "Simple          14         7          50.0      %\n",
            "Academic        61         26         57.4      %\n",
            "Social Media    22         10         54.5      %\n",
            "News            51         25         51.0      %\n",
            "Product Review  47         18         61.7      %\n"
          ]
        }
      ],
      "source": [
        "# Step 17: Comprehensive Analysis Across Text Types\n",
        "print(\"üî¨ Comprehensive Preprocessing Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test standard preprocessing on all text types\n",
        "results = {}\n",
        "for name, text in sample_texts.items():\n",
        "    original_tokens = len(word_tokenize(text))\n",
        "    processed_tokens = preprocess_text(text,\n",
        "                                      clean_level='basic',\n",
        "                                      remove_stopwords=True,\n",
        "                                      use_lemmatization=True)\n",
        "\n",
        "    reduction = (original_tokens - len(processed_tokens)) / original_tokens * 100\n",
        "    results[name] = {\n",
        "        'original': original_tokens,\n",
        "        'processed': len(processed_tokens),\n",
        "        'reduction': reduction,\n",
        "        'sample': processed_tokens[:8]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìÑ {name}:\")\n",
        "    print(f\"   Original: {original_tokens} tokens\")\n",
        "    print(f\"   Processed: {len(processed_tokens)} tokens ({reduction:.1f}% reduction)\")\n",
        "    print(f\"   Sample: {processed_tokens[:8]}\")\n",
        "\n",
        "# Summary table\n",
        "print(f\"\\n\\nüìã Summary Table\")\n",
        "print(f\"{'Text Type':<15} {'Original':<10} {'Processed':<10} {'Reduction':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results.items():\n",
        "    print(f\"{name:<15} {data['original']:<10} {data['processed']:<10} {data['reduction']:<10.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY3hCn_7eV-A"
      },
      "source": [
        "### ü§î Final Conceptual Question 13\n",
        "**Looking at the comprehensive analysis results across all text types:**\n",
        "\n",
        "1. **Which text type was most affected by preprocessing?** Why do you think this happened?\n",
        "\n",
        "2. **Which text type was least affected?** What does this tell you about the nature of that text?\n",
        "\n",
        "3. **If you were building an NLP system to analyze customer reviews for a business, which preprocessing approach would you choose and why?**\n",
        "\n",
        "4. **What are the main trade-offs you need to consider when choosing preprocessing techniques for any NLP project?**\n",
        "\n",
        "**Results:**\n",
        "\n",
        "Simple: 60 ‚Üí 30 tokens (50.0% reduction).\n",
        "Academic: 77 ‚Üí 33 tokens (57.1% reduction).\n",
        "Social Media: 25 ‚Üí 14 tokens (44.0% reduction).\n",
        "News: 60 ‚Üí 29 tokens (51.7% reduction).\n",
        "Product Review: 42 ‚Üí 19 tokens (54.8% reduction).\n",
        "\n",
        "**1. Most affected text type:**\n",
        "\n",
        "Academic: 57.1% reduction.\n",
        "\n",
        "Reason: Academic text (Step 3, Page 5) has complex sentences, stop words (e.g., \"the\", \"of\"), and formal structures. Standard preprocessing (basic cleaning, stop word removal, lemmatization) removes these, significantly reducing tokens (e.g., \"machine-learning\" ‚Üí \"machinelearning\", \"published\" ‚Üí \"publish\"). The high initial token count and dense information amplify the reduction.\n",
        "\n",
        "**2. Least affected text type:**\n",
        "\n",
        "Social Media: 44.0% reduction.\n",
        "\n",
        "Reason: Social media text is short, informal, and has fewer stop words (e.g., \"OMG! Just tried...\"). It contains emojis and hashtags, but basic cleaning retains some (e.g., hashtags as words). This suggests the text is concise with less \"noise\" to remove, making it less sensitive to preprocessing.\n",
        "\n",
        "**3. For customer review analysis:**\n",
        "\n",
        "Standard Processing: (Basic cleaning, stop word removal, lemmatization).\n",
        "\n",
        "Reasoning: Reviews (Step 3, Page 5) contain sentiment-heavy terms (e.g., \"fantastic\") and metrics (e.g., \"4.5/5\"). Standard processing preserves key terms via lemmatization (e.g., \"using\" ‚Üí \"use\"), removes noise (stop words, punctuation), but avoids over-aggressive stemming or emoji removal. This supports sentiment analysis by focusing on meaningful words while retaining sentiment cues.\n",
        "\n",
        "**4. Main trade-offs to consider:**\n",
        "\n",
        "- Information vs. Noise: Aggressive preprocessing reduces noise but risks losing context (e.g., emojis, numbers). Minimal preprocessing preserves information but retains noise, increasing computation.\n",
        "\n",
        "- Accuracy vs. Speed: Lemmatization (accurate) is slower than stemming (fast), impacting real-time applications.\n",
        "\n",
        "- Task-Specificity: Stop word removal suits classification but harms translation. Cleaning levels (basic vs. advanced) must match text type.\n",
        "\n",
        "- Resource Constraints: Complex pipelines (e.g., spaCy) improve accuracy but require more memory than NLTK.\n",
        "\n",
        "\n",
        "\n",
        "**Reference:** https://spacy.io/usage/processing-pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb97qokeV-G"
      },
      "source": [
        "## üéØ Lab Summary and Reflection\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of NLP preprocessing techniques.\n",
        "\n",
        "### üîë Key Concepts You've Mastered:\n",
        "\n",
        "1. **Text Preprocessing Fundamentals** - Understanding why preprocessing is crucial\n",
        "2. **Tokenization Techniques** - NLTK vs spaCy approaches and their trade-offs\n",
        "3. **Stop Word Management** - When to remove them and when to keep them\n",
        "4. **Morphological Processing** - Stemming vs lemmatization for different use cases\n",
        "5. **Text Cleaning Strategies** - Basic vs advanced cleaning for different text types\n",
        "6. **Pipeline Design** - Building modular, configurable preprocessing systems\n",
        "\n",
        "### üéì Real-World Applications:\n",
        "These techniques form the foundation for search engines, chatbots, sentiment analysis, document classification, machine translation, and information extraction systems.\n",
        "\n",
        "### üí° Key Insights to Remember:\n",
        "- **No Universal Solution**: Different NLP tasks require different preprocessing approaches\n",
        "- **Trade-offs Are Everywhere**: Balance information preservation with noise reduction\n",
        "- **Context Matters**: The same technique can help or hurt depending on your use case\n",
        "- **Experimentation Is Key**: Always test and measure impact on your specific task\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work completing Lab 02!** üéâ\n",
        "\n",
        "For your reflection journal, focus on the insights you gained about when and why to use different techniques, the challenges you encountered, and connections you made to real-world applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}